{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287716b2-c90a-45e7-b583-4ccf3beebaa2",
   "metadata": {},
   "source": [
    "# 自注意力机制相关知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc165f1-5524-49b2-91b0-950a63842bdf",
   "metadata": {},
   "source": [
    "### 一、一些疑问："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0686f4ed-459a-4792-8c42-274167964bb3",
   "metadata": {},
   "source": [
    "1、为什么要做QKV线性变换：  \n",
    "为了让模型学会不同类型的信息表示，通过不同的变换，我们可以让查询、键、值专注于各自最有用的信息。  \n",
    "Key 空间：强调“这个词能被什么查询匹配”（如“apple”在水果 vs 公司语境）  \n",
    "Query 空间：强调“当前词想关注什么”（如“eat”会关注食物类 Key）  \n",
    "Value 空间：强调“如果被关注，应该传递什么信息”（如“apple”的水果属性）  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db72b0-0c06-4e00-b14b-c3f5d4969a70",
   "metadata": {},
   "source": [
    "2、为什么要除以sqrt(d_k):  \n",
    "防⽌Softmax函数的梯度消失  \n",
    "点积的数值大小问题：维度d_k越大，点积的值波动越大，绝对值也越大。  \n",
    "softmax问题：当输入的绝对值很大时，softmax 的输出会趋近于 0 或 1（即进入“饱和区”），此时梯度非常小，导致反向传播时梯度消失，模型难以有效学习。  \n",
    "缩放的作用：控制方差，除以sqrt(d_K)的依据是原始QK方差为d_k，而QK/sqrt(d_K)的方差是1，使得softmax的输入保持在一个合理的范围内。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b144ef-edce-4143-8371-4018847dfea7",
   "metadata": {},
   "source": [
    "3、为什么要用softmax：    \n",
    "将相似度分数转化为概率分布  \n",
    "归一化：Softmax确保所有注意力权重之和为1，这样输出就是输入的加权平均。   \n",
    "非负性：Softmax输出都是正数，符合\"注意力程度\"的直观理解。  \n",
    "可解释性：输出值可以直接解释为\"关注程度\"。  \n",
    "可微性：Softmax是可导的，便于反向传播训。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f907b6c-f9e9-4c87-970c-daa17c023ac4",
   "metadata": {},
   "source": [
    "4、为什么要使用多头注意力机制：  \n",
    "与其使用一个头，不如使用多个头并行的关注不同的信息子空间。    \n",
    "线性投影：将输入分别投影到 个不同的子空间  \n",
    "并行计算：在每个子空间中独立计算注意力  \n",
    "拼接输出：将所有头的输出拼接起来  \n",
    "最终线性变换：将拼接结果映射到期望的输出维度  \n",
    "优势是：不同的表示子空间，增强模型的表达能力，稳定训练。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c3dbc-4614-404e-9835-34c66c3d85e7",
   "metadata": {},
   "source": [
    "### 二、自注意力机制的时空复杂度："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182654e-6abe-4694-9435-a9468c5fc770",
   "metadata": {},
   "source": [
    "时间复杂度  \n",
    "1、计算QKV矩阵：：O(n·d_model²)  \n",
    "2、多头分割：O(n·d_model)  \n",
    "3、注意力分数计算：O(n²·d_k) = O(n²·d_model/num_heads)  \n",
    "4、注意力权重计算：O(n²·num_heads)  \n",
    "5、注意力加权：O(n²·d_k) = O(n²·d_model/num_heads)  \n",
    "6、输出投影：O(n·d_model²)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad23b8-0c36-41c5-9dfd-1aedef75efa2",
   "metadata": {},
   "source": [
    "总时间复杂度：O(n·d_model² + n²·d_model/num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c847c3-3516-4212-9b0e-f94ee86f103f",
   "metadata": {},
   "source": [
    "空间复杂度：    \n",
    "1、输入存储：O(n·d_model)  \n",
    "2、QKV变换矩阵存储：O(d_model²)  \n",
    "3、QKV中间结果存储：O(n·d_model)  \n",
    "4、多头分割后存储：O(n·d_model)  \n",
    "5、注意力矩阵存储：O(n²·num_heads)  \n",
    "6、输出存储：O(n·d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d805082b-5118-49f5-bbff-f1e5bdfba32d",
   "metadata": {},
   "source": [
    "总空间复杂度：O(d_model² + n·d_model + n²·num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d44afc-134e-4777-85cc-f50a2a26d290",
   "metadata": {},
   "source": [
    "### 三、多头注意力机制（MHA）代码演示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd30c82-a5e3-4154-9880-962044f19e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([32, 100, 512])\n",
      "输出形状: torch.Size([32, 100, 512])\n",
      "注意力权重形状: torch.Size([32, 8, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model #Linear（self,in,out)输入维度只传入一个参数是在说明“我对每个向量的 d_model 维特征做线性变换，不管有多少个向量。”\n",
    "        self.W_k = nn.Linear(d_model, d_model) #Linear是向量处理器，设计哲学是它不处理“整个矩阵的运算”，而是处理“每个向量的变换”。\n",
    "        self.W_v = nn.Linear(d_model, d_model) #会自动对每一个 d_model 维的向量做相同的线性变换\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):#此处的querykeyvalue都是同一个值“输入向量x”\n",
    "        batch_size = query.size(0)#.size(0)获取第0维的大小，即batch_size\n",
    "\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2) #self.W_q(query) 会触发 self.W_q 实例的 __call__ 方法。\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2) #进而调用其 forward 方法。\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2) #对输入 query 的每一个向量进行线性变换，返回一个形状相同的输出张量 Q。\n",
    "\n",
    "        #调用函数计算注意力分数\n",
    "        attention_output,attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        #拼接\n",
    "        #.transpose()：只改“怎么读数据”（逻辑索引），不改“数据在哪”（物理顺序）\n",
    "        #.contiguous()：把数据复制一份，按新的逻辑顺序重新排列在内存中\n",
    "        #.view()只能对连续存储的数据进行操作\n",
    "        concat_attention = attention_output.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        #最终线性变换\n",
    "        output = self.W_o(concat_attention)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def  scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        #注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32)) \n",
    "        #把 d_k 变成张量，不是因为“不能算”，而是因为“要融入计算图”。\n",
    "\n",
    "        #应用填充掩码\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9) #masked_fill(mask,value)把输入张量中，对应 mask 为 True 的位置，全部替换成 value。\n",
    "\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "\n",
    "# 使用示例\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "# 输入序列\n",
    "seq_len = 100\n",
    "batch_size = 32\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "# 前向传播\n",
    "output, attention_weights = mha(x, x, x)\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785e47b-9716-440c-b8f4-d07dbd00b87a",
   "metadata": {},
   "source": [
    "### 四、多头注意力机制的复杂度分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be95d69-5cad-4b54-a777-f7d0df3e2de0",
   "metadata": {},
   "source": [
    "多头注意力通过并行计算，实现了\"看起来\"有多个注意力机制，但实际上并没有显著增加计算复杂度。   \n",
    "1、参数总量不变  \n",
    "2、可以并行计算  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc3817-4724-47cf-9d81-54e89b9e8acd",
   "metadata": {},
   "source": [
    "**复杂度对比：**    \n",
    "n：序列长度  \n",
    "d_model：模型隐藏维度  \n",
    "num_heads：注意力头数  \n",
    "d_k = d_model / num_heads：每个注意力头的维度      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9369b275-e526-4735-a92b-add5e2e1d8e1",
   "metadata": {},
   "source": [
    "![图片无法显示](../../image/MHA复杂度分析.png \"MHA复杂度分析表\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa8bff-a541-4855-9eeb-6b6d2440b0cf",
   "metadata": {},
   "source": [
    "**为什么复杂度相同但性能更好？**  \n",
    "1. 并行计算：num_heads个小矩阵可以并行计算，而单头的大矩阵计算受限于硬件  \n",
    "2. 内存访问模式：连续的⼩块内存访问比大块稀疏访问更高效  \n",
    "3. 表达能力：多个头可以学习不同的注意力模式，表达能力更强  \n",
    "4. 梯度流：多个独立的注意力路径有助于梯度传播，训练更稳定  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e427825-fd8b-4de6-bb8d-b948b6f901a2",
   "metadata": {},
   "source": [
    "**实际效率提升：**  \n",
    "1. 缓存效率：并行计算更好地利用了GPU的并行能力   \n",
    "2. 内存访问模式：连续的数据访问模式更高效  \n",
    "3. 指令级并⾏：现代CPU/GPU的SIMD指令集  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7c2f3-58b0-4991-b179-f717f01d35be",
   "metadata": {},
   "source": [
    "### 五、GQA注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762859b1-6748-490f-ba4f-b1f798c6367f",
   "metadata": {},
   "source": [
    "相当于将多头分为多个小组，然后按照小组来进行查询，每一个小组共用一个多头。    \n",
    "将查询头分组，每组共享键和值头，在保持性能的同时减少计算量。  \n",
    "核心思想：   \n",
    "多个查询头共享相同的键和值  \n",
    "减少KVcache的内存占用  \n",
    "保持查询的多样性  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fa7d0bd-7590-4ac2-9459-96bfe6416d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GQA输出形状: torch.Size([32, 1000, 512])\n",
      "注意力权重形状: torch.Size([32, 32, 1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_kv_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.head_group_size = num_heads // num_kv_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model // num_heads * num_kv_heads)\n",
    "        self.W_v = nn.Linear(d_model, d_model // num_heads * num_kv_heads)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        sql_len = query.size(1)\n",
    "\n",
    "        #计算OKV\n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        K = self.W_k(key).view(batch_size, seq_len, self.num_kv_heads, self.d_k)\n",
    "        V = self.W_v(value).view(batch_size, seq_len, self.num_kv_heads, self.d_k)\n",
    "\n",
    "        #扩展kv\n",
    "        K = K.unsqueeze(2).expand(-1, -1, self.head_group_size, -1, -1)\n",
    "        V = V.unsqueeze(2).expand(-1, -1, self.head_group_size, -1, -1)\n",
    "\n",
    "        #重塑张量\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        #计算注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "\n",
    "        #填充掩码\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0,-1e9)\n",
    "\n",
    "        #计算注意力权重\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        #计算注意力输出\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        #重塑输出，合并多头\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        #学会多头信息\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# 使用示例\n",
    "d_model = 512\n",
    "num_heads = 32\n",
    "num_kv_heads = 8  # 4:1的压缩比\n",
    "gqa = GroupedQueryAttention(d_model, num_heads, num_kv_heads)\n",
    "# 输入\n",
    "batch_size = 32\n",
    "seq_len = 1000\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "# 前向传播\n",
    "output, attention_weights = gqa(x, x, x)\n",
    "print(f\"GQA输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71949494-cc9b-4c71-a9ec-91a09ca4a7fa",
   "metadata": {},
   "source": [
    "**时空复杂度分析：**  \n",
    "见教案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2715f250-e8d9-42b5-9238-6e2a9a0dc995",
   "metadata": {},
   "source": [
    "### 六、MQA注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683dd3e5-0212-4f8f-9374-7caf62d28c2f",
   "metadata": {},
   "source": [
    "GQA的极端情况，所有查询头共享相同的键和值，进⼀步减少内存占⽤。  \n",
    "核⼼思想：  \n",
    "所有查询头共享⼀个键头和⼀个值头  \n",
    "最⼤化内存效率  \n",
    "可能牺牲⼀些模型质量  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab1795-461f-47bd-8072-fa382b24f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, self.d_k)  # 单个键头\n",
    "        self.W_v = nn.Linear(d_model, self.d_k)  # 单个值头\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "        \n",
    "        # 计算Q (多个头)\n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        Q = Q.transpose(1, 2)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 计算K和V (单个头)\n",
    "        K = self.W_k(key).unsqueeze(1)  # (batch_size, 1, seq_len, d_k)\n",
    "        V = self.W_v(value).unsqueeze(1)  # (batch_size, 1, seq_len, d_k)\n",
    "        \n",
    "        # 扩展K和V以匹配所有查询头\n",
    "        K = K.expand(-1, self.num_heads, -1, -1)\n",
    "        V = V.expand(-1, self.num_heads, -1, -1)\n",
    "        \n",
    "        # 计算注意力\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=to\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # 重塑输出\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    " # 使用示例\n",
    "d_model = 512\n",
    "num_heads = 32\n",
    "mqa = MultiQueryAttention(d_model, num_heads)\n",
    "# 输入\n",
    "batch_size = 32\n",
    "seq_len = 1000\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "# 前向传播\n",
    "output, attention_weights = mqa(x, x, x)\n",
    "print(f\"MQA输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92305c91-3063-4279-bfc4-6d77bc65e6b2",
   "metadata": {},
   "source": [
    "### 七、MHA，GQA，MQA的复杂度分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57cae4d-bb54-4b6b-b3b4-dfe27c2f27f1",
   "metadata": {},
   "source": [
    "见教案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38079bf-b2e3-4d4e-bd6e-f25243048915",
   "metadata": {},
   "source": [
    "### 八、MLA的学习（Multi-Head Latent Attention）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c73e79-6fd2-44e7-a154-a7641f4b4577",
   "metadata": {},
   "source": [
    "#### MLA的核心思想：（共享低维kv+查询解码）  \n",
    "**共享低维 Key/Value (K/V)**：所有注意力头共享一组低维的、数量极少的 K/V 向量（称为 kv_dim，远小于 d_k）。  \n",
    "**Latent KV Cache**\t：这组低维 K/V 被缓存，称为 Latent KV Cache，大小仅为传统 KV Cache 的 1/10 甚至更小。  \n",
    "**Query-to-KV 解码器** ：每个 Query 头通过一个小型解码器网络（通常是 1x1 卷积或线性层），从共享的 Latent K/V 中“解码”出该头专属的 K/V。  \n",
    "**动态生成 K/V**\t：K/V 不是直接投影得到，而是根据当前 Query 动态生成，实现“按需生成”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c8f53c-a162-4c01-aa07-adc6c19e0d0c",
   "metadata": {},
   "source": [
    "#### MLA的优势  \n",
    "**极低 KV Cache**\t只缓存少量 Latent K/V，极大减少推理内存。  \n",
    "**高计算效率**\t解码器轻量，整体计算量低于 MHA。   \n",
    "**保持表达能力**\t通过解码器，不同头仍能生成差异化的 K/V，保持多头多样性。  \n",
    "**适合长上下文**\tKV Cache 小，支持更长的上下文窗口（如 128K）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2099dc-f121-4e86-a402-ef10569c4fe1",
   "metadata": {},
   "source": [
    "#### MoE动态解码器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834d0eaa-dc62-45c3-8767-3b85e71e1507",
   "metadata": {},
   "source": [
    "核心思想：用“专家”（Experts）的集合来替代Transformer标准前馈网络（Feed-Forward Network, FFN）层，并通过一个“门控网络”（Gating Network）智能地为每个输入token选择最合适的专家或专家组合。这样做的主要目的是在不显著增加计算量的前提下，极大地增加模型的参数量，从而提升模型的容量和性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2564a677-e539-41fc-b2ac-49e90476b3c3",
   "metadata": {},
   "source": [
    "#### MoE核心组件：  \n",
    "1、专家（experts）：  \n",
    "专家们本质上是多个独立的前馈神经网络（FFN），在传统transformer中，每一层只有一个FFN，而在MoE层中，这个FFN被替换为了一个包含N个专家的集合。  \n",
    "每一个专家通常与FFN结构相同，但彼此独立参数不共享。  \n",
    "**在训练和推理的时候，不激活所有专家**，这使得模型在拥有大量参数的同时，每一次前向传播只使用其中一小部分，控制了计算成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa24c3-6632-40aa-884e-daea8ffcbcfe",
   "metadata": {},
   "source": [
    "2、门控网络（Gating Network）：  \n",
    "这是一个小型的神经网络（通常是一个线性变换加上一个softmax或者top-k），其输入是当前token的隐藏状态（即前一层的输出）  \n",
    "它的作用是为当前token计算一个权重向量，里面的每一个元素对应了一个专家，表示该专家对该token的重要性（专家被该token选择的概率）。  \n",
    "门控网络是输出经过处理（如top-k）后，决定哪些专家会被激活以及他们的权重。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fa1ed2-9efc-4bbe-935d-aab316939579",
   "metadata": {},
   "source": [
    "3、路由器（Router）：  \n",
    "路由器是门控网络和专家之间的调度器。它根据门控网络的输出，决定将输入token路由（发送）到哪个或哪些专家进行计算。  \n",
    "最常见的策略是Top-k路由，其中k通常为1或2。  \n",
    "Top-1：选择权重最高的那个专家，只有该专家处理这个token。  \n",
    "Top-2：选择权重最高的两个专家，这两个专家都处理这个token，但最终的输出是它们输出的加权和（权重来自门控网络）。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b79d66-23b2-40fe-bf90-146176f44c11",
   "metadata": {},
   "source": [
    "#### MLA代码学习：（不正确，不完善）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceb6157e-8352-43c2-8551-e32ff49e43fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (20x1024 and 512x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 196\u001b[39m\n\u001b[32m    193\u001b[39m mask = torch.tril(torch.ones(\u001b[32m10\u001b[39m, \u001b[32m10\u001b[39m)).unsqueeze(\u001b[32m0\u001b[39m).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m    194\u001b[39m position_ids = torch.arange(\u001b[32m10\u001b[39m).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m output, weights = \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)    \u001b[38;5;66;03m# torch.Size([2, 10, 512])\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWeights shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweights.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# torch.Size([2, 8, 10, 10])\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda\\Anaconda3\\envs\\myshixunenvironment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda\\Anaconda3\\envs\\myshixunenvironment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mMultiHeadLatentAttention.forward\u001b[39m\u001b[34m(self, query, key, value, mask, position_ids)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# --- Step 2: Low-rank Joint Compression of K and V ---\u001b[39;00m\n\u001b[32m     55\u001b[39m KV = torch.cat([key, value], dim=-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (b, s, 2 * d_model)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m KV_compressed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mW_kv_compressed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKV\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (b, s, latent_kv_heads * kv_dim * 2)\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# 分离 latent K and V\u001b[39;00m\n\u001b[32m     59\u001b[39m kv_latent_dim = \u001b[38;5;28mself\u001b[39m.latent_kv_heads * \u001b[38;5;28mself\u001b[39m.kv_dim\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda\\Anaconda3\\envs\\myshixunenvironment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda\\Anaconda3\\envs\\myshixunenvironment\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Anaconda\\Anaconda3\\envs\\myshixunenvironment\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (20x1024 and 512x256)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    完整版 Multi-Head Latent Attention (MLA) Module\n",
    "    核心技术：低秩联合压缩、矩阵吸收、位置编码解耦、MoE 解码器\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, latent_kv_heads=8, kv_dim=64, num_experts=8, top_k=2):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.latent_kv_heads = latent_kv_heads # Latent K/V 的头数（远小于 num_heads）\n",
    "        self.kv_dim = kv_dim # 每个 Latent K/V 的维度（远小于 d_k）\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_experts = num_experts # MoE 解码器的专家数量\n",
    "        self.top_k = top_k # 每次激活的专家数量\n",
    "\n",
    "        # 1. Query 投影\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 2. 低秩联合压缩：K 和 V 联合投影到低维空间\n",
    "        self.W_kv_compressed = nn.Linear(d_model, latent_kv_heads * kv_dim * 2)  # 因为要同时输出压缩后的 K 和 V，所以乘以 2。\n",
    "\n",
    "        # 3. MoE 解码器：Query 条件化生成 K/V 参数\n",
    "        self.k_decoder = MixtureOfExpertsDecoder(\n",
    "            d_model=d_model,\n",
    "            latent_dim=latent_kv_heads * kv_dim,\n",
    "            output_dim=self.num_heads * self.d_k,\n",
    "            num_experts=num_experts,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        self.v_decoder = MixtureOfExpertsDecoder(\n",
    "            d_model=d_model,\n",
    "            latent_dim=latent_kv_heads * kv_dim,\n",
    "            output_dim=self.num_heads * self.d_k,\n",
    "            num_experts=num_experts,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        # 4. 输出投影\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, position_ids=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "    \n",
    "        # --- Step 1:计算Query ---\n",
    "        Q = self.W_q(query)  # (b, s, d_model)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k)  # (b, s, h, d_k)\n",
    "        Q = Q.transpose(1, 2)  # (b, h, s, d_k)\n",
    "    \n",
    "        # --- Step 2: Low-rank Joint Compression of K and V ---\n",
    "        KV = torch.cat([key, value], dim=-1)  # (b, s, 2 * d_model)\n",
    "        KV_compressed = self.W_kv_compressed(KV)  # (b, s, latent_kv_heads * kv_dim * 2)\n",
    "    \n",
    "        # 分离 latent K and V\n",
    "        kv_latent_dim = self.latent_kv_heads * self.kv_dim\n",
    "        K_latent = KV_compressed[:, :, :kv_latent_dim]  # (b, s, L * d_l)\n",
    "        V_latent = KV_compressed[:, :, kv_latent_dim:]  # (b, s, L * d_l)\n",
    "    \n",
    "        K_latent = K_latent.view(batch_size, seq_len, self.latent_kv_heads, self.kv_dim)  # (b, s, L, d_l)\n",
    "        V_latent = V_latent.view(batch_size, seq_len, self.latent_kv_heads, self.kv_dim)  # (b, s, L, d_l)\n",
    "    \n",
    "        # --- Step 3: 从 Query提取条件信号 ---\n",
    "        query_cond = query.mean(dim=1)  # 对序列维度（seq_len）取平均，得到一个代表整个输入序列语义的向量。(b, d_model)\n",
    "    \n",
    "        # 分别生成用于重构 K 和 V 的完整变换矩阵参数。\n",
    "        # Output: (b, num_heads * kv_dim * d_k)\n",
    "        k_params_flat = self.k_decoder(query_cond)  # 使用统一的 decoder\n",
    "        v_params_flat = self.v_decoder(query_cond)\n",
    "    \n",
    "        # Reshape to (b, num_heads, kv_dim, d_k)\n",
    "        k_transform = k_params_flat.view(batch_size, self.num_heads, self.kv_dim, self.d_k)\n",
    "        v_transform = v_params_flat.view(batch_size, self.num_heads, self.kv_dim, self.d_k)\n",
    "    \n",
    "        # --- Step 4: Expand Latent K/V to match num_heads ---\n",
    "        assert self.num_heads % self.latent_kv_heads == 0, \"num_heads must be divisible by latent_kv_heads\"\n",
    "        group_size = self.num_heads // self.latent_kv_heads # 检查并计算“分组大小”。\n",
    "    \n",
    "        # 将每个潜在头复制 group_size 份，Expand latent K/V: (b, s, L, d_l) -> (b, s, L, group_size, d_l) -> (b, s, H, d_l)\n",
    "        K_latent_exp = K_latent.unsqueeze(3).expand(-1, -1, -1, group_size, -1)  # (b, s, L, g, d_l)\n",
    "        V_latent_exp = V_latent.unsqueeze(3).expand(-1, -1, -1, group_size, -1)  # (b, s, L, g, d_l)\n",
    "    \n",
    "        # Reshape to (b, s, H, d_l)\n",
    "        K_latent_flat = K_latent_exp.reshape(batch_size, seq_len, self.num_heads, self.kv_dim)  # (b, s, H, d_l)\n",
    "        V_latent_flat = V_latent_exp.reshape(batch_size, seq_len, self.num_heads, self.kv_dim)  # (b, s, H, d_l)\n",
    "    \n",
    "        # --- Step 5: 使用变换矩阵解码出最终的 K 和 V ---\n",
    "        # 动态解码，执行还原的过程，用爱因斯坦求和函数进行快速维度还原\n",
    "        K = torch.einsum('bsHd_l, bHd_lD_k -> bsHD_k', K_latent_flat, k_transform)  # (b, s, H, d_k)\n",
    "        V = torch.einsum('bsHd_l, bHd_lD_k -> bsHD_k', V_latent_flat, v_transform)\n",
    "    \n",
    "        # Transpose for attention: (b, H, s, d_k)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "    \n",
    "        # --- Step 6: 应用Rope ---\n",
    "        if position_ids is not None:\n",
    "            Q = self.apply_rope(Q, position_ids)  # 只应用于最终的 Q/K\n",
    "            K = self.apply_rope(K, position_ids)\n",
    "    \n",
    "        # --- Step 7: Scaled Dot-Product Attention ---\n",
    "        # (b, h, s, d_k) @ (b, h, d_k, s) -> (b, h, s, s)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "        # --- Step 8: 合并多头并输出 ---\n",
    "        output = output.transpose(1, 2).contiguous()  # (b, s, h, d_k)\n",
    "        output = output.view(batch_size, seq_len, self.d_model)  # (b, s, d_model)\n",
    "        output = self.W_o(output)  # Final projection\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    # 实现Rope位置编码\n",
    "    def apply_rope(self, x, position_ids):\n",
    "        \"\"\"Apply Rotary Position Embedding to x. Simplified version.\"\"\"\n",
    "        # Placeholder for RoPE implementation\n",
    "        # In practice, this would involve sine/cosine rotations\n",
    "        return x\n",
    "\n",
    "# MoE 解码器：根据 Query 条件动态选择专家生成 K/V 参数\n",
    "class MixtureOfExpertsDecoder(nn.Module):\n",
    "    def __init__(self, d_model, latent_dim, output_dim, num_experts=8, top_k=2): # latent_dim专家输入的压缩维度\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.output_dim = output_dim # 最终输出维度\n",
    "\n",
    "        # 专家网络：每个专家负责生成一部分参数\n",
    "        # nn.ModuleList 是 PyTorch 中用来管理多个子模块的容器。\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Linear(latent_dim, output_dim)\n",
    "            for _ in range(num_experts) # 创建num_head个专家\n",
    "        ])\n",
    "        # 门控网络：决定激活哪些专家\n",
    "        self.gate = nn.Linear(d_model, num_experts) # 输出是 num_experts 个分数\n",
    "\n",
    "    def forward(self, query_cond):\n",
    "        gate_logits = self.gate(query_cond)  # (b, num_experts)\n",
    "        gate_scores = torch.softmax(gate_logits, dim=-1)\n",
    "        topk_weights, topk_indices = torch.topk(gate_scores, self.top_k, dim=-1)  # (b, top_k)\n",
    "    \n",
    "        # Step 1: 预计算所有专家的输出: (num_experts, batch_size, output_dim)\n",
    "        expert_outputs = torch.stack([expert(query_cond) for expert in self.experts], dim=0)\n",
    "        # expert_outputs[i, b] 表示第 i 个专家对第 b 个样本的输出\n",
    "    \n",
    "        # Step 2: 初始化输出\n",
    "        batch_size = query_cond.size(0)\n",
    "        output = torch.zeros(batch_size, self.output_dim, device=query_cond.device, dtype=query_cond.dtype)\n",
    "    \n",
    "        # Step 3: 对每个 top-k 位置，gather 对应专家的输出并加权\n",
    "        for k in range(self.top_k):\n",
    "            # 当前是第 k 个被选中的专家（在 top-k 中的位置）\n",
    "            expert_idx = topk_indices[:, k]  # (b,), 每个样本选择的专家编号\n",
    "            weights = topk_weights[:, k].unsqueeze(-1)  # (b, 1)\n",
    "    \n",
    "            # 使用高级索引 gather: (num_experts, b, d) -> (b, d)\n",
    "            # 我们要取 expert_outputs[expert_idx[b], b] for each b\n",
    "            indices = expert_idx.unsqueeze(-1).unsqueeze(-1).expand(-1, 1, self.output_dim)\n",
    "            k_th_expert_output = torch.gather(expert_outputs, 0, indices).squeeze(1)  # (b, d)\n",
    "    \n",
    "            # 加权累加\n",
    "            output += k_th_expert_output * weights\n",
    "    \n",
    "        return output\n",
    "\n",
    "# --- 矩阵吸收技术（在推理前预处理）---\n",
    "def absorb_matrices(mla_module):\n",
    "    \"\"\"\n",
    "    将 W_kv_compressed 与 MoE 专家的权重相乘，实现矩阵吸收\n",
    "    注意：这是一个概念性实现，实际吸收需根据具体结构设计\n",
    "    \"\"\"\n",
    "    # 示例：W_kv_compressed (d_model, L*d_l*2) -> experts (L*d_l, out_dim)\n",
    "    # Absorbed: W_kv_compressed @ expert_weight\n",
    "    # This is complex and typically done during model export/optimization.\n",
    "    # Here we just note the concept.\n",
    "    print(\"Matrix absorption would happen here during model optimization.\")\n",
    "    return mla_module\n",
    "\n",
    "# --- 示例用法 ---\n",
    "if __name__ == \"__main__\":\n",
    "    d_model, num_heads, l_kv_heads, kv_dim = 512, 8, 2, 64\n",
    "    attn = MultiHeadLatentAttention(d_model, num_heads, l_kv_heads, kv_dim)\n",
    "\n",
    "    query = key = value = torch.randn(2, 10, d_model)\n",
    "    mask = torch.tril(torch.ones(10, 10)).unsqueeze(0).unsqueeze(0)\n",
    "    position_ids = torch.arange(10).unsqueeze(0)\n",
    "\n",
    "    output, weights = attn(query, key, value, mask, position_ids)\n",
    "    print(f\"Output shape: {output.shape}\")    # torch.Size([2, 10, 512])\n",
    "    print(f\"Weights shape: {weights.shape}\")  # torch.Size([2, 8, 10, 10])\n",
    "\n",
    "    # 应用矩阵吸收\n",
    "    absorbed_attn = absorb_matrices(attn)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myshixunenvironment)",
   "language": "python",
   "name": "myshixunenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
