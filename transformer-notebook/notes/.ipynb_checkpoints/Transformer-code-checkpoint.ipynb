{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab85f71-62b8-4f06-9473-5455f5ad7628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87980c-04b5-4d7a-860e-265b916ed7df",
   "metadata": {},
   "source": [
    "**词嵌入和位置编码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39af76c8-9da2-47ab-8b31-739eb24a31a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)#vocab_size是词汇表大小\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)  # 缩放\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):#max_len是最大序列长度\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)#创建一个全0矩阵\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()#构建位置索引，unsqueeze(1)的作用是转为列向量\n",
    "        #位置按奇数偶数编码\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() *-(math.log(10000.0) / d_model))#计算公式中的分母\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)#所有行，每隔两列从第0列开始 → 偶数列\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)#所有行，每隔两列从第1列开始 → 奇数列\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)#注册为buffer,把 pe当作模型的一部分保存下来不是可训练参数（不需要梯度）,会随模型一起被保存、加载、移动到 GPU\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]#self.pe是预先计算好的位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277b1bec-043f-427c-8725-07b9003feebe",
   "metadata": {},
   "source": [
    "**注意力机制：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a90dbaa-8816-436d-b489-89306bed5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None, dropout=0.0):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(~mask, -1e9)\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    if dropout > 0.0:\n",
    "        attn_weights = F.dropout(attn_weights, p=dropout, training=True)\n",
    "    output = torch.matmul(attn_weights, value)\n",
    "    return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e3b3f-9010-4f87-9cc0-9fe7aab87145",
   "metadata": {},
   "source": [
    "**多头注意力机制：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "898c3c7d-1452-4eeb-91de-e2a8607ab871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0 #断言：d_model 必须能被 num_heads 整除\n",
    "        self.d_k = d_model // num_heads #计算每一个头的维度\n",
    "        self.num_heads = num_heads #头的数量\n",
    "        self.w_q = nn.Linear(d_model, d_model) \n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model) #\n",
    "        self.w_o = nn.Linear(d_model, d_model) #输出的线性变换\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2) \n",
    "        #.query是线性投影\"（Linear Projection），目的是让模型学习如何生成适合注意力机制的 Query 向量。\n",
    "        #.view() 是重塑张量形状的操作，也可以理解为分头操作\n",
    "        #.transpose(1,2)是交换维度，因为我们要对每个头独立计算注意力，所以要把 num_heads 放在 seq_len 前面。\n",
    "        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # 注意力机制\n",
    "        attn_output, _ = scaled_dot_product_attention(Q, K, V, mask, self.dropout_p)\n",
    "\n",
    "        # 连接多头\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
    "\n",
    "        # 输出的线性变换\n",
    "        output = self.w_o(attn_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c09b9-7cc1-403e-b683-14cf9e2679f9",
   "metadata": {},
   "source": [
    "**前馈网络：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "432c2b35-2c24-48ff-a3f3-068079f6e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.0):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7cf28d-4bee-4e15-98e4-ade669d5844b",
   "metadata": {},
   "source": [
    "**编码器层（单个）：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae735778-a04a-45c6-be8b-59b197406a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.0):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout) #多头自注意力机制\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout) #FFN\n",
    "        self.norm1 = nn.LayerNorm(d_model) \n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        attn_out = self.self_attn(x, x, x, mask) #自注意力机制\n",
    "        x = self.norm1(x + self.dropout(attn_out)) #残差连接Residual和归一化操作\n",
    "\n",
    "        ffn_out = self.ffn(x) #FFN操作\n",
    "        x = self.norm2(x + self.dropout(ffn_out)) #残差连接Residual和归一化操作\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e23fe92-11a5-469f-8648-2ce5c6cb7236",
   "metadata": {},
   "source": [
    "**解码器层（单个）：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99051d9e-8ecb-401c-9317-3cbbe5e17d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.0):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
    "        self_attn_out = self.self_attn(x, x, x, tgt_mask)#自注意力机制\n",
    "        x = self.norm1(x + self.dropout(self_attn_out))\n",
    "        \n",
    "        enc_dec_attn_out = self.enc_dec_attn(x, enc_out, enc_out, src_mask)#交叉注意力机制\n",
    "        x = self.norm2(x + self.dropout(enc_dec_attn_out))\n",
    "\n",
    "        ffn_out = self.ffn(x) #FFN\n",
    "        x = self.norm3(x + self.dropout(ffn_out))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916dd94e-7d4e-49ed-9fc2-8270b51c932e",
   "metadata": {},
   "source": [
    "**编码器层和解码器层（整体）：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "406ab36e-afe8-4e3f-95bf-e905aff62e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size, dropout=0.0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = TokenEmbedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        x = self.embedding(src)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size, dropout=0.0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = TokenEmbedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, enc_out, src_mask, tgt_mask):\n",
    "        x = self.embedding(tgt)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, src_mask, tgt_mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a455b10-d9b3-4144-97b9-01eef85f1298",
   "metadata": {},
   "source": [
    "**transformer模型：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1944b83-f200-4320-871e-98014f4b7c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, num_layers=6, d_ff=2048, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, d_ff, src_vocab_size, dropout)\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, d_ff, tgt_vocab_size, dropout)\n",
    "        self.output_projection = nn.Linear(d_model, tgt_vocab_size) #输出模块\n",
    "\n",
    "    #填充掩码\n",
    "    def make_src_mask(self, src, pad_idx):\n",
    "        return (src != pad_idx).unsqueeze(-2).unsqueeze(-2)  # (batch, 1, 1, seq_len)\n",
    "\n",
    "    #目标序列掩码（填充掩码+前瞻掩码）\n",
    "    def make_tgt_mask(self, tgt, pad_idx):\n",
    "        tgt_pad_mask = (tgt != pad_idx).unsqueeze(-2).unsqueeze(-2)  # (batch, 1, 1, seq_len)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_sub_mask = torch.tril(torch.ones(tgt_len, tgt_len)).bool().to(tgt.device)\n",
    "        return tgt_pad_mask & tgt_sub_mask  # (batch, 1, seq_len, seq_len)\n",
    "\n",
    "    def forward(self, src, tgt, src_pad_idx, tgt_pad_idx):\n",
    "        src_mask = self.make_src_mask(src, src_pad_idx)#先创建掩码\n",
    "        tgt_mask = self.make_tgt_mask(tgt, tgt_pad_idx)\n",
    "\n",
    "        enc_out = self.encoder(src, src_mask)\n",
    "        dec_out = self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
    "        logits = self.output_projection(dec_out)  # (batch, seq, vocab_size 把 d_model 维的隐藏状态映射到 vocab_size 维的“未归一化对数概率”（即 logits）\n",
    "        probs = F.log_softmax(logits, dim=-1)#输出每一个词的概率\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989d186-3a71-440c-8fc3-40fcc37971e2",
   "metadata": {},
   "source": [
    "**接下来是示例运行：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "817990f1-3666-480f-bcea-fbdea64cc08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入源序列形状: torch.Size([2, 10])\n",
      "输入目标序列形状: torch.Size([2, 12])\n",
      "输出概率分布形状: torch.Size([2, 12, 1000])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 模型参数\n",
    "    SRC_VOCAB_SIZE = 1000 #源语言词表大小\n",
    "    TGT_VOCAB_SIZE = 1000 #目标语言词表大小\n",
    "    D_MODEL = 512\n",
    "    NUM_HEADS = 8\n",
    "    NUM_LAYERS = 6\n",
    "    D_FF = 2048\n",
    "    DROPOUT = 0.1\n",
    "    PAD_IDX = 0 #padding token 的索引\n",
    "\n",
    "    # 构建模型\n",
    "    model = Transformer(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, D_MODEL, NUM_HEADS, NUM_LAYERS, D_FF, DROPOUT)\n",
    "\n",
    "    # 示例输入\n",
    "    batch_size = 2\n",
    "    src_seq_len = 10\n",
    "    tgt_seq_len = 12\n",
    "\n",
    "    src = torch.randint(1, SRC_VOCAB_SIZE, (batch_size, src_seq_len))  # (batch, seq_len)\n",
    "    tgt = torch.randint(1, TGT_VOCAB_SIZE, (batch_size, tgt_seq_len)) #生成一个形状为 [2, 10] 的张量，每个元素是 1 到 999 之间的随机整数（token ID），从 1 开始是为了避开 PAD_IDX=0\n",
    "\n",
    "    # 前向传播\n",
    "    output_probs = model(src, tgt, PAD_IDX, PAD_IDX)\n",
    "\n",
    "    print(\"输入源序列形状:\", src.shape)\n",
    "    print(\"输入目标序列形状:\", tgt.shape)\n",
    "    print(\"输出概率分布形状:\", output_probs.shape)  # 应为 (batch, tgt_seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f4a61c-f265-4379-a0ff-3a8f004dc53f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myshixunenvironment)",
   "language": "python",
   "name": "myshixunenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
