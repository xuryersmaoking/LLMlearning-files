{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c9c1084-ad61-4385-a5ae-fee9e92e7104",
   "metadata": {},
   "source": [
    "# 推理相关学习笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eaf9c8-73bf-405b-8b2a-b2106bfff6de",
   "metadata": {},
   "source": [
    "## 一、KVcache相关"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025d6ac-d14f-4ebd-9ca7-31ede687aab5",
   "metadata": {},
   "source": [
    "**KVcache原理：**  \n",
    "是大预言模型推理过程中用于存储和复用注意力机制中Key和Value向量的技术。  \n",
    "在Transformer的自回归生成过程中，每生成一个新的token，都需要重新计算所有已生成token的注意力。通过KV-Cache，我们可以缓存之前计算过的Key和Value向量，避免重复计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ee568-2601-41ae-a46e-044c4f584a27",
   "metadata": {},
   "source": [
    "**使用KVcache的原因：**  \n",
    "1、显著提升计算效率，避免重复计算KV。  \n",
    "2、内存优化，减少重复矩阵计算，提升缓存命中率（从缓存中取出历史KV，从而不用去主存中重新计算，缓存命中率 = 命中次数 / 总访问次数）。  \n",
    "3、时间复杂度优化，没有KV-Cache时，每轮生成的时间复杂度为O(n²)，其中n是当前序列长度，用了后降为O(n)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b9275-0e41-4b67-aad5-31a643159f24",
   "metadata": {},
   "source": [
    "**为什么只缓存KV，不缓存Q？**  \n",
    "1、Query的特殊性：    \n",
    "Q向量是基于当前正在生成的token计算的每个新token都有不同的Q向量。  \n",
    "Q向量无法复用，因为它们对应不同的生成位置。  \n",
    "2、计算需求：  \n",
    "在每轮生成中，我们只需要当前token的Q向量但需要所有历史token的KV向量来计算注意力权重。  \n",
    "3、存储效率：  \n",
    "只缓存KV可以减少约1/3的存储需求。   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308ce58-d1e1-4c59-a959-f6966410b3ba",
   "metadata": {},
   "source": [
    "## 二、VLLM（Virtual Large Language Model）框架相关"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde8d15-4c4c-4ba2-9851-20123336fb45",
   "metadata": {},
   "source": [
    "**定义：**  \n",
    "是一个开源的大语言模型推理和服务框架，专注于提高大模型推理的吞吐量和内存效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbe382-fd23-426c-9f5a-81afe30cbca1",
   "metadata": {},
   "source": [
    "**特点及优势：**  \n",
    "1、PagedAttention：创新的注意力机制实现，优化内存管理。      \n",
    "2、连续批处理：动态批处理技术，提高硬件利用率。   \n",
    "3、高效的KV-Cache管理：智能的缓存管理机制。  \n",
    "4、量化支持：支持多种量化技术以减少内存占用。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25fa94-a7e4-46ff-abe8-352a1cbafa63",
   "metadata": {},
   "source": [
    "VLLM提升了内存效率，吞吐量，延迟显著降低。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b16c7a-52b3-415f-94e1-dc497726be14",
   "metadata": {},
   "source": [
    "**传统KVcache的问题（解释了为什么需要PagedAttention）:**  \n",
    "1、内存碎片化：  \n",
    "不同序列的KV-Cache大小不同。随着序列增长，内存分配不连续导致内存碎片化（传统的KVcache要连续的内存空间），降低内存利用率。  \n",
    "2、内存预分配困难：  \n",
    "需要为每个序列预分配最大可能的内存。浪费大量内存空间，限制了并发序列数量。  \n",
    "3、动态序列管理复杂：  \n",
    "序列长度动态变化，内存重新分配成本高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0279ec-a2fd-4794-ac52-3ae482e79cb1",
   "metadata": {},
   "source": [
    "**PagedAttention解决方案：**  \n",
    "借鉴了操作系统的虚拟内存和分页管理思想。  \n",
    "1、物理块（Physical Block）：  \n",
    "固定大小的内存块（如16个token的KV-Cache）预先分配，避免动态分配。  \n",
    "2、逻辑块（Logical Block）：  \n",
    "序列的逻辑视图可以映射到任意物理块。  \n",
    "3、块表（Block Table）：  \n",
    "记录逻辑块到物理块的映射关系，支持非连续内存访问。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc9790-a65d-49f3-b2ac-cda58339e92d",
   "metadata": {},
   "source": [
    "**PagedAttention的工作原理：**  \n",
    "分页机制：\n",
    "1. 块大小定义：通常设置为16个token（具体取决于硬件优化）每个块包含16个Key向量和16个Value向量。  \n",
    "2. 内存分配：预先分配大量固定大小的物理块维护空闲块列表。  \n",
    "3. 映射管理：每个序列维护一个块表块表记录逻辑块到物理块的映射。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476897b-0252-4c8c-ab80-b40ebf3b7248",
   "metadata": {},
   "source": [
    "注意力计算优化：  \n",
    "1. 块级并行：可以并行处理不同的物理块，提高硬件利用率。  \n",
    "2. 内存访问优化：连续的物理块访问。   \n",
    "3. 动态扩展：序列增长时只需分配新块，无需重新分配整个KV-Cache（与传统预分配大小不足导致重新分配形成对比） 。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b17b7eb-3e97-4f9f-8826-7bd78aa0b89a",
   "metadata": {},
   "source": [
    "**PagedAttention性能优化方向：**  \n",
    "1、内存利用率优化  \n",
    "减少内存碎片：通过分页机制，内存利用率可提升90%以上。  \n",
    "动态内存管理：根据实际需求分配内存块。  \n",
    "内存共享：相同前缀的序列可以共享物理块。  \n",
    "2、计算效率优化  \n",
    "块级并行计算：利用GPU的并行计算能力。  \n",
    "内存访问优化：连续内存访问提高缓存效率。  \n",
    "减少数据拷贝：通过映射而非数据移动实现块管理。  \n",
    "3、批处理优化   \n",
    "跨序列批处理：不同序列的块可以组成批处理。  \n",
    "动态批处理：根据序列长度动态调整批处理策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29133c1-33b6-418a-a7db-d82f45b41e09",
   "metadata": {},
   "source": [
    "**连续批处理（Continuous Batching）：**  \n",
    "连续批处理是种动态批处理技术，它将不同时间到达的请求动态组合成批次进行处理，而不是等待固定大小的批次 。                                                            \n",
    "\n",
    "工作原理：  \n",
    "1、请求队列管理：  \n",
    "维护待处理请求队列，根据优先级和资源需求排序。  \n",
    "2、动态批次构建：  \n",
    "实时评估可组合的请求，构建最优批次。  \n",
    "3、资源调度：  \n",
    "根据硬件资源动态调整批次大小，平衡吞吐量和延迟。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2498e-03af-4c4c-a139-023b1c5eefa6",
   "metadata": {},
   "source": [
    "对吞吐量的优化：  \n",
    "1、硬件利用率提升：减少GPU空闲时间提高计算单元利用率。  \n",
    "2、减少等待时间：不必等待完整批次，快速响应新请求。  \n",
    "3、自适应优化：根据负载动态调整，在吞吐量和延迟间找到平衡。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8207c295-5e10-444f-b5d1-cce0007f6a54",
   "metadata": {},
   "source": [
    "**与传统批处理的差别：**  \n",
    "连续批处理动态组合请求，传统批处理等待固定数量请求到达。  \n",
    "连续批处理批次大小可变，传统批处理批次大小固定。  \n",
    "连续批处理减少等待时间，传统批处理有等待延迟。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290055b4-b172-4ab5-b205-c98fd4ea1895",
   "metadata": {},
   "source": [
    "## 三、解码策略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5980bf6-6572-43f4-8a6f-74c15f156a54",
   "metadata": {},
   "source": [
    "在大预言模型的文本生成过程中，解码策略决定了如何从模型输出的概率分布中选择下一个token。不同的解码策略会产生不同质量的文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b1a38f-53b7-49bf-b227-e71154d55137",
   "metadata": {},
   "source": [
    "**贪婪搜索（Greedy Search）：**  \n",
    "原理：每一步都选择概率最高的token。  \n",
    "公式：$w_t=arg \\max\\limits_w P(w|w_1,w_2...w_{t-1})$  \n",
    "优点：计算简单，速度快，结果确定性高。  \n",
    "缺点：容易陷入重复模式，缺乏创造性。  \n",
    "适用场景：需要确定性输出，对速度要求高的场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53516bb9-79c0-4f28-ade7-9b864b65b70b",
   "metadata": {},
   "source": [
    "**束搜索（Beam Search）：**  \n",
    "原理：由宽度优先搜索BFS启发而来，维护k个最有可能的序列，每一步扩展所有序列并保留前k个最优的。 \n",
    "\n",
    "基本流程：   \n",
    "1、初始化：从起始符号（如`<s>`）开始了，有k个相同的候选序列（都只包含`<s>`）。  \n",
    "2、扩展（Expand）：对于当前保留的 K 个候选序列中的每一个，模型生成下一个 token 的概率分布。将每个候选序列扩展为 V 个新序列（V 是词汇表大小），即在原序列末尾分别加上词汇表中的每一个 token。  \n",
    "3、剪枝（Prune）：现在我们有 K * V 个候选序列。我们计算每个新序列的总得分（通常是 token 概率的乘积，或更常见的对数概率之和），然后只保留得分最高的 K 个序列。  \n",
    "4、重复：用这 K 个新序列作为下一步的输入，重复“扩展”和“剪枝”过程，直到所有序列都生成了结束符或达到最大长度。  \n",
    "5、最终，从所有已完成的序列中，选择总得分最高的那个作为最终输出。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb3d009-0c9c-46ad-b5c0-b66f7f138f78",
   "metadata": {},
   "source": [
    "优点：  \n",
    "比贪婪搜索更全局优化，通常产生更合理的文本。  \n",
    "\n",
    "缺点：  \n",
    "计算复杂度高，可能产生重复或过于保守的文本。  \n",
    "\n",
    "适用场景：  \n",
    "机器翻译，文本摘要。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5ecd57-25ae-4e90-9660-c2a88bace6f3",
   "metadata": {},
   "source": [
    "**Top-K采样：**  \n",
    "原理：从概率最高的k个token中随机采样。    \n",
    "\n",
    "过程：  \n",
    "1、计算所有token的概率分布。  \n",
    "2、选择概率最高的k个token。   \n",
    "3、重新归一化这k个token的概率。  \n",
    "4、从中随机采样。   \n",
    "\n",
    "优点：  \n",
    "引入随机性，增加创造性。  \n",
    "避免低质量token。 \n",
    "\n",
    "缺点：  \n",
    "k值固定，不够灵活，无法适应不同分布。    \n",
    "在分布平坦时可能包含太多低质量token。  \n",
    "在分布尖锐时可能遗漏高质量token。  \n",
    "\n",
    "适用场景：  \n",
    "文本生成，创意写作。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755866cc-b1fa-4586-a402-4e380b4f3e6c",
   "metadata": {},
   "source": [
    "**Top-p（Nucleus）采样：**  \n",
    "原理：从累积概率超过p的最小token集合中随机采样。  \n",
    "\n",
    "过程：  \n",
    "1、计算所有token的概率分布。    \n",
    "2、按概率降序排序。   \n",
    "3、选择累积概率首次超过p的最小集合。  \n",
    "4、重新归一化集合内token的概率。    \n",
    "5、从中随机采样。  \n",
    "\n",
    "优点：  \n",
    "动态调整候选集合大小，适应不同的概率。  \n",
    "更灵活，能平衡质量和创造性。\n",
    "\n",
    "缺点：    \n",
    "需要调整p值，计算复杂度略高。  \n",
    "\n",
    "适用场景：  \n",
    "对话生成，故事创作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52679a81-7441-4668-bb60-85b0f58203e0",
   "metadata": {},
   "source": [
    "**温度参数：**  \n",
    "定义：温度参数是控制概率分布锐度的超参数，用于调节生成文本的随机性和创造性。  \n",
    "\n",
    "标准softmax（T=1）公式：\n",
    "$$P(i) = \\frac{\\exp(z_i)}{\\sum_{j=1}^{V} \\exp(z_j)}$$\n",
    "广义上的温度公式：\n",
    "$$P_T(i) = \\frac{\\exp(z_i / T)}{\\sum_{j=1}^{V} \\exp(z_j / T)}$$  \n",
    "其中，$z_i$是$token_i$的logits(没有归一化的概率)，T是温度参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a73bc4-9813-400c-af69-018fb6bb52cc",
   "metadata": {},
   "source": [
    "**不同温度带来的影响：**  \n",
    "1、T=0（贪婪解码）：  \n",
    "概率分布变成one-hot分布，总是选择概率最高的token输出，确定性最高。  \n",
    "2、0<T<1 （低温）：    \n",
    "增强高概率token的优势。  \n",
    "减弱低概率token的影响。  \n",
    "输出更加确定性和保守。  \n",
    "3、T=1（原始分布）：  \n",
    "不改变原始概率分布，保持模型训练时的分布特性。  \n",
    "4、T>1（高温）：  \n",
    "减弱高概率token的优势。  \n",
    "增强低概率token的影响。  \n",
    "概率更平滑，输出更加随机和创造性。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31719d4d-6994-4dbb-ad03-3c084b5654ee",
   "metadata": {},
   "source": [
    "**温度参数对解码策略的影响：**  \n",
    "1、与贪婪搜索结合：  \n",
    "温度参数不影响贪婪搜索结果（总是选择最高概率）。  \n",
    "2、与采样策略结合：  \n",
    "低温：使Top-k和Top-p采样更保守。  \n",
    "高温：使Top-k和Top-p采样更随机。  \n",
    "3、实际应用建议：    \n",
    "创意写作：适用较高温度（1.2-1.5）  \n",
    "技术文档：适用较低温度（0.5-0.8）  \n",
    "对话系统：适用中等温度（0.8-1.2）  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e06dc-048c-4f1e-bda8-f3532eb880d8",
   "metadata": {},
   "source": [
    "**推测解码（Speculative Decoding）：**  \n",
    "推测解码是⼀种加速大预言模型推理的技术，通过使用⼀个小模型（草稿模型）来推测多个token，然后由大模型（目标模型）并行验证这些推测。  \n",
    "\n",
    "工作原理：  \n",
    "1、推测阶段：  \n",
    "适用快速的⼩模型生成多个token的草稿序列，通常生成2-8个token。  \n",
    "2、验证阶段：   \n",
    "适用大模型并行计算草稿序列中每个位置的概率分布，比较草稿token和大模型的预测。  \n",
    "3、接受/拒绝：  \n",
    "从第⼀个被拒绝的token开始，截断草稿序列，适用大模型生成下一个token。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88e9ed-4ca6-4473-8c9d-93e3d6387ac3",
   "metadata": {},
   "source": [
    "推测解码的加速效果来源于：  \n",
    "1、并行验证：⼀次验证多个token，而非逐个生成减少了大模型的调用次数。    \n",
    "2、快速推测：草稿模型计算速度快大部分计算由小模型完成。    \n",
    "3、接受率优化：设计良好的草稿模型有较高接受率减少重新生成的开销。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba67b35-0b88-4845-a28f-82e383452634",
   "metadata": {},
   "source": [
    "实际应用考虑：  \n",
    "1、草稿模型设计：需要与目标模型保持一定一致性。    \n",
    "太相似：加速效果有限  \n",
    "太不同：接受率低  \n",
    "2、硬件优化：  \n",
    "可以在不同设备上并行运行草稿模型和目标模型，进⼀步提升效率。  \n",
    "3、适用场景：  \n",
    "生成较长文本时效果更明显，对延迟敏感的应用场景。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7775d1-0965-432b-a464-798387ce5e12",
   "metadata": {},
   "source": [
    "## 四、预填充（Prefill）阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088b679-893c-422e-a46a-360b754be5f6",
   "metadata": {},
   "source": [
    "Prefill（预填充）是大语言模型推理过程中的第⼀个阶段，负责处理用户输入的提示词（prompt）并生成第⼀个输出token。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3f988-a6ea-4764-bb5f-d3e5bc6a57d1",
   "metadata": {},
   "source": [
    "**Prefill阶段的特点：**  \n",
    "1、计算密集：  \n",
    "需要处理整个提示词序列，计算复杂度与序列长度的平方成正比。  \n",
    "2、内存密集：  \n",
    "需要存储整个提示词的KV-Cache，内存使用量随序列长度线性增长。  \n",
    "3、⼀次性操作：  \n",
    "每个推理请求只执行一次，为后续的Decode阶段做准备。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55641641-c85a-4984-943c-83160f067de5",
   "metadata": {},
   "source": [
    "**prefill和decode的区别：**   \n",
    "见教案  \n",
    "Prefill阶段的计算量远大于单轮Decode，但Prefill只执行⼀次，而Decode需要执行多次。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d70c8e-a928-48c5-82d3-053cc480b2d9",
   "metadata": {},
   "source": [
    "**PD分离（Prefill-Decode Separation）：**    \n",
    "见教案  \n",
    "⼀种系统架构设计，将Prefill和Decode阶段部署在不同的硬件资源上，以优化整体性能。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db519e53-868f-43b8-a9f0-e5534d7c485c",
   "metadata": {},
   "source": [
    "**为什么分成Prefill和Decode两个阶段：**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ccaed9-7e78-480e-b5f4-fc0872a31bc6",
   "metadata": {},
   "source": [
    "见教案"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myshixunenvironment)",
   "language": "python",
   "name": "myshixunenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
